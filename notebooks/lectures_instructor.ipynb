{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Lectures, First Year Project 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor's version</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1, ITU Copenhagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructor: Michael Szell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course page: https://learnit.itu.dk/local/coursebase/view.php?ciid=590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook wrangles and explores the data set from the project.\n",
    "\n",
    "Contact: Michael Szell (misz@itu.dk)  \n",
    "Created: 2021-01-11  \n",
    "Last modified: 2021-02-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: First data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Add later as needed\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add later as needed\n",
    "%run -i ../scripts/functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are written all caps: https://www.python.org/dev/peps/pep-0008/#constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {}\n",
    "PATH[\"data_raw\"] = \"../data/raw/\"\n",
    "PATH[\"data_interim\"] = \"../data/interim/\"\n",
    "PATH[\"data_processed\"] = \"../data/processed/\"\n",
    "PATH[\"data_external\"] = \"../data/external/\"\n",
    "\n",
    "FILENAME = {}\n",
    "FILENAME[\"accidents\"] = \"Road Safety Data - Accidents 2019.csv\"\n",
    "FILENAME[\"casualties\"] = \"Road Safety Data - Casualties 2019.csv\"\n",
    "FILENAME[\"vehicles\"] = \"Road Safety Data- Vehicles 2019.csv\" # Note the inconsistent file naming (no space before \"-\" here)\n",
    "\n",
    "# Add later, lec 2\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data were downloaded from here on Jan 4th: https://data.gov.uk/dataset/road-accidents-safety-data  \n",
    "That page was updated afterwards (Jan 8th), so local and online data may be inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explore one data table, the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version, just using the accident table\n",
    "dataraw_acc = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"accidents\"], delimiter=',', dtype=None, names=True, encoding='utf8')\n",
    "\n",
    "# # Add later, end of lec 1\n",
    "# dataraw = {}\n",
    "# dataraw[\"accidents\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"accidents\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "# dataraw[\"vehicles\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"vehicles\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "# dataraw[\"casualties\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"casualties\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "\n",
    "# Add later, in lec 2\n",
    "dataraw = {}\n",
    "for tablename in TABLENAMES:\n",
    "    dataraw[tablename] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[tablename], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add later, in lec 2\n",
    "headerraw = {}\n",
    "for tablename in TABLENAMES:\n",
    "    headerraw[tablename] = list(dataraw[tablename].dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start with a \"sneak preview\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataraw_acc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder and documentation on structured arrays:  \n",
    "https://numpy.org/devdocs/user/basics.rec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Mixed variable types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accidents have mixed data types, including strings, floats, integers. Categorical variables are encoded as integers. The meaning of these categories can be looked up in `../references/variable lookup.xls`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_acc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataraw_acc.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_acc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Data in the wild\" puzzle: Why is the first field \"\\ufeffAccident_Index\" and not \"Accident_Index\"?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataraw_acc.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: utf8 was the wrong encoding! The correct one seems to be utf-8-sig.\n",
    "\n",
    "https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string/17912811#17912811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor: Go back to top and fix. Also make dict of tables. Explain code refactoring:</font>  \n",
    "https://en.wikipedia.org/wiki/Code_refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: Explore the other two tables the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Command line wrangling and dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster way of getting basic insights into a new data set than by using numpy is by using command line tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a first overview using `head`. There are 3 data tables: Accidents, Casualties, and Vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head -n 6 \"../data/raw/Road Safety Data - Accidents 2019.csv\" \n",
    "!head -n 6 \"../data/raw/Road Safety Data - Casualties 2019.csv\"\n",
    "!head -n 6 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link between data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records between data tables are linked through their `Accident_Index`.\n",
    "\n",
    "Looking at the first Accident_Index 2019010128300, we can see there seems to be a one-to-many relation between accident->casualty and accident->vehicle, meaning there can be multiple casualties and vehicles involved in one accident (makes sense).\n",
    "\n",
    "https://en.wikipedia.org/wiki/One-to-many_(data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Wc_(Unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l \"../data/raw/Road Safety Data - Accidents 2019.csv\" \n",
    "!wc -l \"../data/raw/Road Safety Data - Casualties 2019.csv\" \n",
    "!wc -l \"../data/raw/Road Safety Data- Vehicles 2019.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fields (in first line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/awk-command-unixlinux-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -1 \"../data/raw/Road Safety Data - Accidents 2019.csv\" | awk -F ',' '{print NF}'\n",
    "!head -1 \"../data/raw/Road Safety Data - Casualties 2019.csv\" | awk -F ',' '{print NF}'\n",
    "!head -1 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | awk -F ',' '{print NF}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See and count all fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tr_(Unix)\n",
    "https://en.wikipedia.org/wiki/Nl_(Unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 \"../data/raw/Road Safety Data - Accidents 2019.csv\" | tr ',' '\\n' | nl\n",
    "!head -1 \"../data/raw/Road Safety Data - Casualties 2019.csv\" | tr ',' '\\n' | nl\n",
    "!head -1 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | tr ',' '\\n' | nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has each record the same number of fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shapeshed.com/unix-uniq/  \n",
    "https://www.putorius.net/uniq-command-linux.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data - Accidents 2019.csv\" | sort | uniq -d\n",
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data - Casualties 2019.csv\" | sort | uniq -d\n",
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | sort | uniq -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many duplicate lines are there? (If more than 0, there could be a problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sort \"../data/raw/Road Safety Data - Accidents 2019.csv\" | uniq -d  | wc -l\n",
    "!sort \"../data/raw/Road Safety Data - Casualties 2019.csv\" | uniq -d | wc -l\n",
    "!sort \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | uniq -d  | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced stuff with `awk`: https://datafix.com.au/BASHing/2020-05-20.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a masked array:  \n",
    "https://numpy.org/devdocs/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor: We want a nicer way to loop through table names. Let's go to beginning and refactor with TABLENAMES.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_masked = {}\n",
    "for tablename in TABLENAMES:\n",
    "    dataraw_masked[tablename] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[tablename], delimiter=',', dtype=None, names=True, encoding='utf-8-sig', usemask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_masked[\"accidents\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataraw_masked[\"accidents\"].mask[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 rows seem complete. What about the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(dataraw_masked[\"accidents\"].mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh oh, values are missing in 5776 rows! In which rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_incomplete = np.where(dataraw_masked[\"accidents\"].mask)[0]\n",
    "print(rows_incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values in total?  \n",
    "Which fields are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingpositions = {}\n",
    "missingvalues = 0 # Add later\n",
    "missingconfigurations = set() # Add later\n",
    "for rowpos in rows_incomplete:\n",
    "    missingpositions_thisrow = list(np.where(list(dataraw_masked[\"accidents\"].mask[rowpos]))[0])\n",
    "    missingpositions[rowpos] = missingpositions_thisrow\n",
    "    missingvalues += len(missingpositions_thisrow) # Add later\n",
    "    missingconfigurations.add((tuple(missingpositions_thisrow))) # Add later\n",
    "\n",
    "missingfieldnames = [np.array(headerraw[\"accidents\"])[c] for c in [list(b) for b in missingconfigurations]] # Add later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missingpositions) # Don't do this is you have more than a few 1000 rows or Jupyter might crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Incomplete rows: \" + str(np.count_nonzero(dataraw_masked[\"accidents\"].mask)))\n",
    "print(\"Missing values: \" + str(missingvalues))\n",
    "print(\"\\nMissing field configurations: \" + str(missingconfigurations))\n",
    "print(\"Missing field configurations (names): \")\n",
    "for i in missingfieldnames:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Visual data exploration, Connecting tables, Association test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual exploratory data analysis (\"Plot your data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headerraw[\"accidents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = \"Accident_Severity\"\n",
    "field_categories = {1: \"Fatal\", 2: \"Serious\", 3: \"Slight\"}\n",
    "\n",
    "categories, counts = np.unique(dataraw[\"accidents\"][field_name], return_counts=True)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1]) # left, bottom, width, height (range 0 to 1)\n",
    "axes.bar(categories, counts, fc=\"gray\") # fc is the face color\n",
    "\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_title(field_name)\n",
    "\n",
    "axes.set_xticks(list(field_categories.keys()))\n",
    "axes.set_xticklabels(field_categories.values());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were lucky because the categories 1,2,3 were \"nice\". But usually they aren't, so we need to explicitly map to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "field_name = \"Speed_limit\"\n",
    "field_categories = {20: \"20 MPH\", 30: \"30 MPH\", 40: \"40 MPH\", 50: \"50 MPH\", 60: \"60 MPH\", 70: \"70 MPH\", -1: \"N/A\"}\n",
    "\n",
    "categories, counts = np.unique(dataraw[\"accidents\"][field_name], return_counts=True)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1]) # left, bottom, width, height (range 0 to 1)\n",
    "axes.bar(range(len(categories)), counts, fc=\"gray\") # fc is the face color\n",
    "\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_ylabel('Count')\n",
    "axes.set_title(field_name)\n",
    "fig.autofmt_xdate(rotation=45) # Add later\n",
    "\n",
    "axes.set_xticks(range(len(categories)))\n",
    "axes.set_xticklabels([field_categories[c] for c in categories]); # Changing this line is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of copy-pasting code, let's write a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add this line later for testing\n",
    "%run -i ../scripts/functions.py\n",
    "\n",
    "field_name = \"Light_Conditions\"\n",
    "field_categories = {1: \"Daylight\", 4: \"Darkness - lights lit\", 5: \"Darkness - lights unlit\", 6: \"Darkness - no lighting\", 7: \"Darkness - lighting unknown\", -1: \"Data missing or out of range\"}\n",
    "\n",
    "barplot(dataraw[\"accidents\"], field_name, field_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typing the variable lookup manually is cumbersome. Can we read the excel directly?  \n",
    "pandas can: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Add to beginning\n",
    "variable_lookup = pd.read_excel('../references/variable lookup.xls', sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(variable_lookup.keys())\n",
    "print(\"\\n\")\n",
    "print(variable_lookup[\"Introduction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerraw[\"casualties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "axes.hist(dataraw[\"casualties\"][\"Age_of_Casualty\"]); # Add bins later\n",
    "\n",
    "axes.set_ylabel('Counts')\n",
    "axes.set_title(\"Age_of_Casualty\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "axes.hist(dataraw[\"casualties\"][\"Age_of_Casualty\"], bins = np.linspace(-1,121, 123)); # Add bins later\n",
    "\n",
    "axes.set_ylabel('Counts')\n",
    "axes.set_title(\"Age_of_Casualty\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "axes = fig.add_axes([0, 0, 1, 1])\n",
    "axes.hist(dataraw[\"vehicles\"][\"Age_of_Driver\"], bins = np.linspace(-1,121, 123)); # Try again with (0,121, 122)\n",
    "\n",
    "axes.set_ylabel('Counts')\n",
    "axes.set_title(\"Age_of_Driver\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplots are good for relating two numerical variables. If we have one numerical versus one categorical variable, we can do a box plot. But could we also visualize all data points? Yes: https://seaborn.pydata.org/tutorial/categorical.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ind = (dataraw[\"casualties\"][\"Casualty_Type\"] == 0) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 1) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 9) & (dataraw[\"casualties\"][\"Age_of_Casualty\"] > -1)\n",
    "ind[600:] = False\n",
    "data_toplot = np.array([dataraw[\"casualties\"][\"Casualty_Type\"][ind], dataraw[\"casualties\"][\"Age_of_Casualty\"][ind]]).T\n",
    "\n",
    "fig = sns.catplot(x='Casualty Type', y='Age of Casualty', data=pd.DataFrame(data_toplot, columns=['Casualty Type', 'Age of Casualty']), kind=\"swarm\") # also show: violin\n",
    "fig.set_xticklabels([\"Pedestrian\", \"Cyclist\", \"Car occupant\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = (dataraw[\"casualties\"][\"Casualty_Type\"] == 0) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 1) | (dataraw[\"casualties\"][\"Casualty_Type\"] == 9) & (dataraw[\"casualties\"][\"Age_of_Casualty\"] > -1) & (dataraw[\"casualties\"][\"Sex_of_Casualty\"] > -1)\n",
    "ind[600:] = False\n",
    "data_toplot = np.array([dataraw[\"casualties\"][\"Casualty_Type\"][ind], dataraw[\"casualties\"][\"Age_of_Casualty\"][ind], dataraw[\"casualties\"][\"Sex_of_Casualty\"][ind]]).T\n",
    "\n",
    "fig = sns.catplot(x='Casualty Type', y='Age of Casualty', hue='Sex of Casualty', data=pd.DataFrame(data_toplot, columns=['Casualty Type', 'Age of Casualty', 'Sex of Casualty']), kind=\"swarm\") # also show: violin\n",
    "fig.set_xticklabels([\"Pedestrian\", \"Cyclist\", \"Car occupant\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting tables with np.isin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How many babies and toddlers died or got injured on UK roads in June 2019?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get all the accident indices from the accidents table for June\n",
    "2. Filter all casualties for those indices\n",
    "3. Filter those casualties for ages 0-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has the function isin() to select for a list of indices: https://numpy.org/doc/stable/reference/generated/numpy.isin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [x[\"Accident_Index\"] for x in dataraw[\"accidents\"] if \"/06/\" in x[\"Date\"]]\n",
    "datafiltered = dataraw[\"casualties\"][np.isin(dataraw[\"casualties\"][\"Accident_Index\"], ind)]\n",
    "datafiltered = datafiltered[(datafiltered[\"Age_of_Casualty\"] >= 0) & (datafiltered[\"Age_of_Casualty\"] <= 4)]\n",
    "len(datafiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Who killed or injured them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association test between two categorical variables \n",
    "**(Pearson $\\chi^2$ test of independence)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by:  \n",
    "https://peterstatistics.com/CrashCourse/3-TwoVarUnpair/NomNom/NomNom-2a-Test.html  \n",
    "https://bit.ly/3kbwKEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us ask: Is there a statistically significant association between accident severity and speed limit?**  \n",
    "We ask because speed limit is something that the city government can regulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in the realm of [Statistical hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing). In general, we must first state and compare two hypotheses:\n",
    "\n",
    "- $H_0$ (null hypothesis): There is no statistically significant relationship between accident severity and speed limit.\n",
    "- $H_\\alpha$ (alternative hypothesis): There is a statistically significant relationship between accident severity and speed limit.\n",
    "\n",
    "We must then 1) state+check statistical assumptions, 2) choose an appropriate test and test statistic $T$, 3) derive the distribution for the test statistic, 4) select a significance level $\\alpha$, usually 0.01 or 0.05, 5) calculate the observed test statistic $t_{\\mathrm obs}$, 6) calculate the [p-value](https://en.wikipedia.org/wiki/P-value). \n",
    "\n",
    "If the p-value $< \\alpha$, then the null hypothesis will be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson $\\chi^2$ test of independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test association between two categorical variables, one uses the [Pearson chi-square test of independence](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test). If the significance of this test (p-value) is below a significance level (typically 0.05), the two variables have a significant association.\n",
    "\n",
    "The Pearson chi-square test should only be used if most cells have an expected count above 5, and the minimum expected count is at least 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = (dataraw[\"accidents\"][\"Speed_limit\"] != -1)\n",
    "severityspeed = np.array([dataraw[\"accidents\"][\"Speed_limit\"][ind], dataraw[\"accidents\"][\"Accident_Severity\"][ind]]).T\n",
    "print(severityspeed.shape)\n",
    "print(severityspeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We crosstabulate using pandas:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html\n",
    "\n",
    "The cross tabulation is also known as contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pd = pd.crosstab(severityspeed[:, 0], severityspeed[:, 1], rownames = [\"Speed limit\"], colnames = [\"Accident Severity\"]) # To explain expected values, show also with , normalize='index', and , normalize='columns' https://stackoverflow.com/questions/21247203/how-to-make-a-pandas-crosstab-with-percentages\n",
    "observed = observed_pd.to_numpy()\n",
    "observed_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is now to compare these observed values with expected values.  \n",
    "The expected values can be calculated using:\n",
    "\\begin{equation*}\n",
    "E_{i,j} = \\frac{R_i \\times C_j}{N}\n",
    "\\end{equation*}\n",
    "The $E_{i,j}$ indicates the expected count in row i, column j. The $R_i$ is the row total of row i, and $C_j$ the column total of column j. The $N$ is the grand total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.zeros(observed.shape, dtype=int)\n",
    "colTotals = observed.sum(axis=0)\n",
    "rowTotals = observed.sum(axis=1)\n",
    "N = rowTotals.sum()\n",
    "\n",
    "for i in range(observed.shape[0]):\n",
    "    for j in range(observed.shape[1]):\n",
    "        expected[i,j] = (rowTotals[i] * colTotals[j]) / N\n",
    "\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the manual way of doing it. `chi2_contingency()` can do it for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chiVal, pVal, df, expected = chi2_contingency(observed)\n",
    "chiVal, pVal, df, expected.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the p-value `pVal`=0.0 < 0.05, the result is significant: There is a significant association between speed and accident severity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that the association is significant, but how strong is it?  \n",
    "Cramer's V, for example, can give an answer: https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "The formula is:\n",
    "\\begin{equation*}\n",
    "V=\\sqrt{\\frac{\\chi^{2} / N}{\\min (c-1, r-1)}}\n",
    "\\end{equation*}\n",
    "where $c$ is the number of columns, $r$ is the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.sqrt( (chiVal/N) / (min(observed.shape)-1) )\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A V close to 0 implies a weak effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize this and make a human-readable plot and report below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 6))\n",
    "severity_labels = [\"Fatal\", \"Serious\", \"Slight\"]\n",
    "speed_categories = {20: \"20 MPH\", 30: \"30 MPH\", 40: \"40 MPH\", 50: \"50 MPH\", 60: \"60 MPH\", 70: \"70 MPH\"}\n",
    "x = np.array(list(speed_categories.keys()))\n",
    "\n",
    "for i, ax in enumerate(axes[0]):\n",
    "    ax.plot(x, observed[:,i], 'ro-', label='Observed')\n",
    "    ax.plot(x, expected[:,i], 'bo-', label='Expected')\n",
    "    if i==0: \n",
    "        ax.set_ylabel('Casualties')\n",
    "        ax.legend(loc='best');\n",
    "    ax.set_title(severity_labels[i])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(list(speed_categories.values()))\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "for i, ax in enumerate(axes[1]):\n",
    "    ax.plot(x, observed[:,i]/expected[:,i], 'go-')\n",
    "    ax.plot(x, np.ones(x.shape), 'k:')\n",
    "    \n",
    "    if i==0: \n",
    "        ax.set_ylabel('Observed/Expected')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(list(speed_categories.values()))\n",
    "    fig.autofmt_xdate(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An association only tells us about correlation, not causation. However, in this case, there is good reason to say that the speed of cars are a cause of the accident severity. Although Cramer's V is low, there is a very clear effect, especially for fatal collisions, $\\chi^2$(10, N = 117456) = 1951.03, p < .001, V = 0.091.\n",
    "\n",
    "The conclusion is therefore that **\"Speed kills\"**:  \n",
    "Fatal collisions are over 2 times more likely than expected for high speeds (>60 mph), and are 2-3 times less likely than expected for low speed limits (<30 mph). An urban planning policy recommendation would therefore be: To reduce fatalities, reduce speed limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about statistical tests for different combinations of numerical/categorical variables?\n",
    "<img src=\"../references/flowchart-for-choosing-a-statistical-test.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Spatial filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityname = \"Bristol\"\n",
    "cityid = \"bristol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa = np.genfromtxt(PATH[\"data_external\"] + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.csv\", delimiter=',', dtype=None, names=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select all rows for the city name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(lsoa['LSOA11NM'] == cityname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops. We did not find a single row. Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38974168/finding-entries-containing-a-substring-in-a-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_LSOA11CD_rows = np.flatnonzero(np.core.defchararray.find(lsoa['LSOA11NM'],cityname)!=-1)\n",
    "city_LSOA11CD = lsoa['LSOA11CD'][city_LSOA11CD_rows]\n",
    "city_LSOA11CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use this list of LSOA11 codes to restrict our accident data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_accidentindices = dataraw[\"accidents\"][\"Accident_Index\"][np.isin(dataraw[\"accidents\"][\"LSOA_of_Accident_Location\"], city_LSOA11CD)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity = {}\n",
    "for tablename in TABLENAMES:\n",
    "    datacity[tablename] = dataraw[tablename][np.isin(dataraw[tablename][\"Accident_Index\"], city_accidentindices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datacity[\"accidents\"]), len(datacity[\"vehicles\"]), len(datacity[\"casualties\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename in TABLENAMES:\n",
    "    with open(PATH[\"data_interim\"] + tablename + \"_\" + cityid + \".csv\", \"w\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(dataraw[tablename].dtype.names)\n",
    "        w.writerows(datacity[tablename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial filtering with shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor only: Run one time to get JSON</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_external\"] + \"lad.json\") as f:\n",
    "    jsn = json.load(f)\n",
    "cities = {\"Birmingham\": \"Birmingham\", \"Leeds\": \"Leeds\", \"Sheffield\": \"Sheffield\", \"Bradford\": \"Bradford\", \"Liverpool\": \"Liverpool\", \"Manchester\": \"Manchester\", \"Bristol, City of\": \"Bristol\"}\n",
    "\n",
    "cities_json = {}\n",
    "for i in range(len(jsn['features'])):\n",
    "    if jsn['features'][i][\"properties\"][\"LAD13NM\"] in cities:\n",
    "        cities_json[cities[jsn['features'][i][\"properties\"][\"LAD13NM\"]]] = jsn['features'][i]\n",
    "        print(cities[jsn['features'][i][\"properties\"][\"LAD13NM\"]])\n",
    "with open(PATH[\"data_processed\"] + \"citieslad.json\", \"w\") as f:\n",
    "    json.dump(cities_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor only END</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to beginning imports\n",
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_processed\"] + \"citieslad.json\") as f:\n",
    "    cities_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary = shapely.geometry.shape(cities_json[cityname][\"geometry\"])\n",
    "type(city_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter visualizes shapely objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary.geom_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all accident coordinates (from the whole UK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_list = dataraw[\"accidents\"][\"Longitude\"]\n",
    "lat_list = dataraw[\"accidents\"][\"Latitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contains()` and `within()` check for point inclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(city_boundary.contains(Point(lon_list[0], lat_list[0])))\n",
    "print(city_boundary.contains(Point(lon_list[102135], lat_list[102135])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_acc_rowindices = []\n",
    "for i in range(len(lon_list)):\n",
    "    if Point(lon_list[i], lat_list[i]).within(city_boundary):\n",
    "        city_acc_rowindices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_acc_rowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity = {}\n",
    "datacity[\"accidents\"] = dataraw[\"accidents\"][city_acc_rowindices]\n",
    "len(datacity[\"accidents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_interim\"] + \"accidents_\" + cityid + \"lad.csv\", \"w\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(data_acc.dtype.names)\n",
    "    w.writerows(city_data_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit vehicles and casualties to these AccidentIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity[\"vehicles\"] = dataraw[\"vehicles\"][np.isin(dataraw[\"vehicles\"][\"Accident_Index\"], datacity[\"accidents\"][\"Accident_Index\"])]\n",
    "datacity[\"casualties\"] = dataraw[\"casualties\"][np.isin(dataraw[\"casualties\"][\"Accident_Index\"], datacity[\"accidents\"][\"Accident_Index\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datacity[\"accidents\"]), len(datacity[\"vehicles\"]), len(datacity[\"casualties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!diff ../data/interim/accidents_bristol.csv ../data/interim/accidents_bristol_lad.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean?  \n",
    "See: https://www.computerhope.com/unix/udiff.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Visualizing spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these to the imports in the beginning\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://alysivji.github.io/getting-started-with-folium.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = np.vstack((datacity[\"accidents\"]['Latitude'], datacity[\"accidents\"]['Longitude'])).T\n",
    "centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "m1 = folium.Map(centroid, zoom_start=11)\n",
    "for row in datacity[\"accidents\"]:\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "                        radius = 5,\n",
    "                        popup = row['Accident_Index'] + \"\\n\" + row[\"Date\"] + \", \" + row[\"Time\"],\n",
    "                        fill_color = \"#3db7e4\",\n",
    "                       ).add_to(m1)\n",
    "\n",
    "HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m1))\n",
    "folium.LayerControl().add_to(m1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap is built with KDE:  \n",
    "https://en.wikipedia.org/wiki/Kernel_density_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add automatic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = np.vstack( (datacity[\"accidents\"]['Latitude'], datacity[\"accidents\"]['Longitude'])).T\n",
    "centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "m2 = folium.Map(centroid, zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(folium.FeatureGroup(name='Clusters').add_to(m2))\n",
    "for row in datacity[\"accidents\"]:\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "                        radius = 5,\n",
    "                        popup = row['Accident_Index'] + \"\\n\" + row[\"Date\"] + \", \" + row[\"Time\"],\n",
    "                        fill_color = \"#3db7e4\",\n",
    "                       ).add_to(marker_cluster)\n",
    "\n",
    "HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m2))\n",
    "folium.LayerControl().add_to(m2)\n",
    "m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp2021",
   "language": "python",
   "name": "fyp2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
