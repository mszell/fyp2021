{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Lectures, First Year Project 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor's version</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1, ITU Copenhagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructor: Michael Szell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course page: https://learnit.itu.dk/local/coursebase/view.php?ciid=590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook wrangles and explores the data set from the project.\n",
    "\n",
    "Contact: Michael Szell (misz@itu.dk)  \n",
    "Created: 2021-01-11  \n",
    "Last modified: 2021-01-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: First data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Add later as needed\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add later as needed\n",
    "%run -i ../scripts/functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are written all caps: https://www.python.org/dev/peps/pep-0008/#constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = {}\n",
    "PATH[\"data_raw\"] = \"../data/raw/\"\n",
    "PATH[\"data_interim\"] = \"../data/interim/\"\n",
    "PATH[\"data_processed\"] = \"../data/processed/\"\n",
    "PATH[\"data_external\"] = \"../data/external/\"\n",
    "\n",
    "FILENAME = {}\n",
    "FILENAME[\"accidents\"] = \"Road Safety Data - Accidents 2019.csv\"\n",
    "FILENAME[\"casualties\"] = \"Road Safety Data - Casualties 2019.csv\"\n",
    "FILENAME[\"vehicles\"] = \"Road Safety Data- Vehicles 2019.csv\" # Note the inconsistent file naming (no space before \"-\" here)\n",
    "\n",
    "# Add later, lec 2\n",
    "TABLENAMES = [\"accidents\", \"casualties\", \"vehicles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data were downloaded from here on Jan 4th: https://data.gov.uk/dataset/road-accidents-safety-data  \n",
    "That page was updated afterwards (Jan 8th), so local and online data may be inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explore one data table, the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version, just using the accident table\n",
    "dataraw_acc = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"accidents\"], delimiter=',', dtype=None, names=True, encoding='utf8')\n",
    "\n",
    "# # Add later, end of lec 1\n",
    "# dataraw = {}\n",
    "# dataraw[\"accidents\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"accidents\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "# dataraw[\"vehicles\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"vehicles\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "# dataraw[\"casualties\"] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[\"casualties\"], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')\n",
    "\n",
    "# Add later, in lec 2\n",
    "dataraw = {}\n",
    "for tablename in TABLENAMES:\n",
    "    dataraw[tablename] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[tablename], delimiter=',', dtype=None, names=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add later, in lec 2\n",
    "headerraw = {}\n",
    "for tablename in TABLENAMES:\n",
    "    headerraw[tablename] = list(dataraw[tablename].dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start with a \"sneak preview\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataraw_acc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder and documentation on structured arrays:  \n",
    "https://numpy.org/devdocs/user/basics.rec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Mixed variable types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accidents have mixed data types, including strings, floats, integers. Categorical variables are encoded as integers. The meaning of these categories can be looked up in `../references/variable lookup.xls`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_acc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataraw_acc.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_acc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Data in the wild\" puzzle: Why is the first field \"\\ufeffAccident_Index\" and not \"Accident_Index\"?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataraw_acc.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: utf8 was the wrong encoding! The correct one seems to be utf-8-sig.\n",
    "\n",
    "https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string/17912811#17912811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor: Go back to top and fix. Also make dict of tables. Explain code refactoring:</font>  \n",
    "https://en.wikipedia.org/wiki/Code_refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: Explore the other two tables the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Command line wrangling and dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster way of getting basic insights into a new data set than by using numpy is by using command line tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a first overview using `head`. There are 3 data tables: Accidents, Casualties, and Vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head -n 6 \"../data/raw/Road Safety Data - Accidents 2019.csv\" \n",
    "!head -n 6 \"../data/raw/Road Safety Data - Casualties 2019.csv\"\n",
    "!head -n 6 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link between data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records between data tables are linked through their `Accident_Index`.\n",
    "\n",
    "Looking at the first Accident_Index 2019010128300, we can see there seems to be a one-to-many relation between accident->casualty and accident->vehicle, meaning there can be multiple casualties and vehicles involved in one accident (makes sense).\n",
    "\n",
    "https://en.wikipedia.org/wiki/One-to-many_(data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Wc_(Unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l \"../data/raw/Road Safety Data - Accidents 2019.csv\" \n",
    "!wc -l \"../data/raw/Road Safety Data - Casualties 2019.csv\" \n",
    "!wc -l \"../data/raw/Road Safety Data- Vehicles 2019.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of fields (in first line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/awk-command-unixlinux-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -1 \"../data/raw/Road Safety Data - Accidents 2019.csv\" | awk -F ',' '{print NF}'\n",
    "!head -1 \"../data/raw/Road Safety Data - Casualties 2019.csv\" | awk -F ',' '{print NF}'\n",
    "!head -1 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | awk -F ',' '{print NF}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See and count all fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tr_(Unix)\n",
    "https://en.wikipedia.org/wiki/Nl_(Unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 \"../data/raw/Road Safety Data - Accidents 2019.csv\" | tr ',' '\\n' | nl\n",
    "!head -1 \"../data/raw/Road Safety Data - Casualties 2019.csv\" | tr ',' '\\n' | nl\n",
    "!head -1 \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | tr ',' '\\n' | nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has each record the same number of fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://shapeshed.com/unix-uniq/  \n",
    "https://www.putorius.net/uniq-command-linux.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data - Accidents 2019.csv\" | sort | uniq -d\n",
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data - Casualties 2019.csv\" | sort | uniq -d\n",
    "!awk -F ',' '{print NF}' \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | sort | uniq -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many duplicate lines are there? (If more than 0, there could be a problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sort \"../data/raw/Road Safety Data - Accidents 2019.csv\" | uniq -d  | wc -l\n",
    "!sort \"../data/raw/Road Safety Data - Casualties 2019.csv\" | uniq -d | wc -l\n",
    "!sort \"../data/raw/Road Safety Data- Vehicles 2019.csv\" | uniq -d  | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced stuff with `awk`: https://datafix.com.au/BASHing/2020-05-20.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a masked array:  \n",
    "https://numpy.org/devdocs/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor: We want a nicer way to loop through table names. Let's go to beginning and refactor with TABLENAMES.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_masked = {}\n",
    "for tablename in TABLENAMES:\n",
    "    dataraw_masked[tablename] = np.genfromtxt(PATH[\"data_raw\"]+FILENAME[tablename], delimiter=',', dtype=None, names=True, encoding='utf-8-sig', usemask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataraw_masked[\"accidents\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataraw_masked[\"accidents\"].mask[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 rows seem complete. What about the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(dataraw_masked[\"accidents\"].mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh oh, values are missing in 5776 rows! In which rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_incomplete = np.where(dataraw_masked[\"accidents\"].mask)[0]\n",
    "print(rows_incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many values in total?  \n",
    "Which fields are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingpositions = {}\n",
    "missingvalues = 0 # Add later\n",
    "missingconfigurations = set() # Add later\n",
    "for rowpos in rows_incomplete:\n",
    "    missingpositions_thisrow = list(np.where(list(dataraw_masked[\"accidents\"].mask[rowpos]))[0])\n",
    "    missingpositions[rowpos] = missingpositions_thisrow\n",
    "    missingvalues += len(missingpositions_thisrow) # Add later\n",
    "    missingconfigurations.add((tuple(missingpositions_thisrow))) # Add later\n",
    "\n",
    "missingfieldnames = [np.array(headerraw[\"accidents\"])[c] for c in [list(b) for b in missingconfigurations]] # Add later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(missingpositions) # Don't do this is you have more than a few 1000 rows or Jupyter might crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Incomplete rows: \" + str(np.count_nonzero(dataraw_masked[\"accidents\"].mask)))\n",
    "print(\"Missing values: \" + str(missingvalues))\n",
    "print(\"\\nMissing field configurations: \" + str(missingconfigurations))\n",
    "print(\"Missing field configurations (names): \")\n",
    "for i in missingfieldnames:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Distributions, Regression, Connecting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Spatial filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityname = \"Bristol\"\n",
    "cityid = \"bristol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa = np.genfromtxt(PATH[\"data_external\"] + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.csv\", delimiter=',', dtype=None, names=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select all rows for the city name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(lsoa['LSOA11NM'] == cityname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops. We did not find a single row. Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38974168/finding-entries-containing-a-substring-in-a-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_LSOA11CD_rows = np.flatnonzero(np.core.defchararray.find(lsoa['LSOA11NM'],cityname)!=-1)\n",
    "city_LSOA11CD_list = lsoa['LSOA11CD'][city_LSOA11CD_rows] # this is a list\n",
    "city_LSOA11CD = {k: 0 for k in city_LSOA11CD_list.tolist()} # Turn into dict keys. \n",
    "# Alternative: city_LSOA11CD = dict.fromkeys(city_LSOA11CD , 0)\n",
    "city_LSOA11CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use this dict of LSOA11 codes to restrict our accident data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Why use a dict, and not a list?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit 'E01033370' in city_LSOA11CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit 'E01033370' in city_LSOA11CD_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will do some quite restricting of data sets, lets write a function for that, in `functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to beginning of notebook\n",
    "%run -i ../scripts/functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_accidentindices = restrict_dataset(dataraw[\"accidents\"], city_LSOA11CD, \"LSOA_of_Accident_Location\")[\"Accident_Index\"]\n",
    "city_accidentindices = {k: 0 for k in city_accidentindices.tolist()} # Turn into dict keys\n",
    "city_accidentindices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of turning a list into a dict before calling the function, lets accept both. We extend our function:\n",
    "```python\n",
    "if type(indices) == \"list\": \n",
    "    indices = {k: 0 for k in indices}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity = {}\n",
    "for tablename in TABLENAMES:\n",
    "    datacity[tablename] = restrict_dataset(dataraw[tablename], city_accidentindices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename in TABLENAMES:\n",
    "    with open(PATH[\"data_interim\"] + tablename + \"_\" + cityid + \".csv\", \"w\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(dataraw[tablename].dtype.names)\n",
    "        w.writerows(datacity[tablename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial filtering with shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor only: Run one time to get JSON</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_external\"] + \"lad.json\") as f:\n",
    "    jsn = json.load(f)\n",
    "cities = {\"Birmingham\": \"Birmingham\", \"Leeds\": \"Leeds\", \"Sheffield\": \"Sheffield\", \"Bradford\": \"Bradford\", \"Liverpool\": \"Liverpool\", \"Manchester\": \"Manchester\", \"Bristol, City of\": \"Bristol\"}\n",
    "\n",
    "cities_json = {}\n",
    "for i in range(len(jsn['features'])):\n",
    "    if jsn['features'][i][\"properties\"][\"LAD13NM\"] in cities:\n",
    "        cities_json[cities[jsn['features'][i][\"properties\"][\"LAD13NM\"]]] = jsn['features'][i]\n",
    "        print(cities[jsn['features'][i][\"properties\"][\"LAD13NM\"]])\n",
    "with open(PATH[\"data_processed\"] + \"citieslad.json\", \"w\") as f:\n",
    "    json.dump(cities_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Instructor only END</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to beginning imports\n",
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, MultiPoint, LineString, MultiLineString, Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_processed\"] + \"citieslad.json\") as f:\n",
    "    cities_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary = shapely.geometry.shape(cities_json[cityname][\"geometry\"])\n",
    "type(city_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter visualizes shapely objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_boundary.geom_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all accident coordinates (from the whole UK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_list = dataraw[\"accidents\"][\"Longitude\"]\n",
    "lat_list = dataraw[\"accidents\"][\"Latitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`contains()` and `within()` check for point inclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(city_boundary.contains(Point(lon_list[0], lat_list[0])))\n",
    "print(city_boundary.contains(Point(lon_list[102135], lat_list[102135])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_acc_rowindices = []\n",
    "for i in range(len(lon_list)):\n",
    "    if Point(lon_list[i], lat_list[i]).within(city_boundary):\n",
    "        city_acc_rowindices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(city_acc_rowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity = {}\n",
    "datacity[\"accidents\"] = dataraw[\"accidents\"][city_acc_rowindices]\n",
    "datacity[\"accidents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_interim\"] + \"accidents_\" + cityid + \"lad.csv\", \"w\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(data_acc.dtype.names)\n",
    "    w.writerows(city_data_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit casualties and vehicles to these AccidentIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datacity[\"accidents\"][\"Accident_Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_accidentindices = {}\n",
    "for i in range(len(datacity[\"accidents\"][\"Accident_Index\"])):\n",
    "    city_accidentindices[datacity[\"accidents\"][\"Accident_Index\"][i]] = 0\n",
    "city_accidentindices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter vehicles and make an interim data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity[\"vehicles\"] = restrict_dataset(dataraw[\"vehicles\"], city_accidentindices)\n",
    "datacity[\"vehicles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datacity[\"vehicles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_interim\"] + \"vehicles_\" + cityid + \"lad.csv\", \"w\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(dataraw[\"vehicles\"].dtype.names)\n",
    "    w.writerows(datacity[\"vehicles\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter casualties and make an interim data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacity[\"casualties\"] = restrict_dataset(dataraw[\"casualties\"], city_accidentindices)\n",
    "datacity[\"casualties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datacity[\"casualties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH[\"data_interim\"] + \"casualties_\" + cityid + \"lad.csv\", \"w\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(dataraw[\"casualties\"].dtype.names)\n",
    "    w.writerows(datacity[\"casualties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!diff ../data/interim/accidents_bristol.csv ../data/interim/accidents_bristol_lad.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean?  \n",
    "See: https://www.computerhope.com/unix/udiff.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Visualizing spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these to the imports in the beginning\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://alysivji.github.io/getting-started-with-folium.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = np.vstack( (datacity[\"accidents\"]['Latitude'], datacity[\"accidents\"]['Longitude'])).T\n",
    "centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "m1 = folium.Map(centroid, zoom_start=11)\n",
    "for row in datacity[\"accidents\"]:\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "                        radius = 5,\n",
    "                        popup = row['Accident_Index'] + \"\\n\" + row[\"Date\"] + \", \" + row[\"Time\"],\n",
    "                        fill_color = \"#3db7e4\",\n",
    "                       ).add_to(m1)\n",
    "\n",
    "HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m1))\n",
    "folium.LayerControl().add_to(m1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap is built with KDE:  \n",
    "https://en.wikipedia.org/wiki/Kernel_density_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add automatic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = np.vstack( (datacity[\"accidents\"]['Latitude'], datacity[\"accidents\"]['Longitude'])).T\n",
    "centroid = list(MultiPoint(latlons).centroid.coords)[0]\n",
    "m2 = folium.Map(centroid, zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(folium.FeatureGroup(name='Clusters').add_to(m2))\n",
    "for row in datacity[\"accidents\"]:\n",
    "    folium.CircleMarker([row['Latitude'], row['Longitude']],\n",
    "                        radius = 5,\n",
    "                        popup = row['Accident_Index'] + \"\\n\" + row[\"Date\"] + \", \" + row[\"Time\"],\n",
    "                        fill_color = \"#3db7e4\",\n",
    "                       ).add_to(marker_cluster)\n",
    "\n",
    "HeatMap(latlons).add_to(folium.FeatureGroup(name='Heat Map').add_to(m2))\n",
    "folium.LayerControl().add_to(m2)\n",
    "m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp2021",
   "language": "python",
   "name": "fyp2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
